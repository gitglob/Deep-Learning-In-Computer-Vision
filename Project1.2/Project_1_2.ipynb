{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import os\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "import copy\n",
    "import time\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from joblib import Parallel, delayed\n",
    "import selectivesearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change project directory and run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "data_path='/dtu/datasets1/02514/data_wastedetection'\n",
    "project_path = os.path.join(os.getcwd(), 'Project1.2')\n",
    "os.mkdir(os.path.join(project_path, 'data'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'json'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'raw'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'splitted'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'raw', 'test'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'raw', 'train'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'splitted', 'train'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'splitted', 'test'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'splitted', 'train', 'Background'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'splitted', 'test', 'Background'))\n",
    "\n",
    "annotations = json.load(open(os.path.join('Project1.2/annotations.json')))\n",
    "supercategories = {}\n",
    "categories = ['Background']\n",
    "for i in range(len(annotations['categories'])):\n",
    "    supercategories[str(i)] = annotations['categories'][i]['supercategory']\n",
    "    if annotations['categories'][i]['supercategory'] not in categories:\n",
    "        categories.append(annotations['categories'][i]['supercategory'])\n",
    "        os.mkdir(os.path.join(project_path, 'data', 'splitted', 'train', annotations['categories'][i]['supercategory']))\n",
    "        os.mkdir(os.path.join(project_path, 'data', 'splitted', 'test', annotations['categories'][i]['supercategory']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json.load(open(os.path.join('Project1.2/annotations.json')))\n",
    "supercategories = {}\n",
    "categories = ['Background']\n",
    "for i in range(len(annotations['categories'])):\n",
    "    supercategories[str(i)] = annotations['categories'][i]['supercategory']\n",
    "    if annotations['categories'][i]['supercategory'] not in categories:\n",
    "        categories.append(annotations['categories'][i]['supercategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in range(len(annotations['images'])):\n",
    "    if int(annotations['images'][id]['file_name'].split('_')[1].split('/')[0]) < 13:\n",
    "        shutil.copyfile(os.path.join(data_path, annotations['images'][id]['file_name']), os.path.join(project_path, 'data', 'raw', 'train', str(id)+'.jpg'))\n",
    "    else:\n",
    "        shutil.copyfile(os.path.join(data_path, annotations['images'][id]['file_name']), os.path.join(project_path, 'data', 'raw', 'test', str(id)+'.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(box1, box2):\n",
    "    \"\"\" \n",
    "        We assume that the box follows the format:\n",
    "        box1 = [x1,y1,x2,y2], and box2 = [x3,y3,x4,y4],\n",
    "        where (x1,y1) and (x3,y3) represent the top left coordinate,\n",
    "        and (x2,y2) and (x4,y4) represent the bottom right coordinate \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        box1: The coordinates of the first box\n",
    "        box2: The coordinates of the first box\n",
    "        threshold: The boundary threshold\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box1\t\n",
    "    x3, y3, x4, y4 = box2\n",
    "    x_inter1 = max(x1, x3)\n",
    "    y_inter1 = max(y1, y3)\n",
    "    x_inter2 = min(x2, x4)\n",
    "    y_inter2 = min(y2, y4)\n",
    "    if x_inter2 <= x_inter1 or y_inter2 <= y_inter1:\n",
    "        return 0.0\n",
    "    width_inter = abs(x_inter2 - x_inter1)\n",
    "    height_inter = abs(y_inter2 - y_inter1)\n",
    "    area_inter = width_inter * height_inter\n",
    "    width_box1 = abs(x2 - x1)\n",
    "    height_box1 = abs(y2 - y1)\n",
    "    width_box2 = abs(x4 - x3)\n",
    "    height_box2 = abs(y4 - y3)\n",
    "    area_box1 = width_box1 * height_box1\n",
    "    area_box2 = width_box2 * height_box2\n",
    "    area_union = area_box1 + area_box2 - area_inter\n",
    "    iou_res = area_inter / area_union\n",
    "    return  iou_res\n",
    "\n",
    "def resize(image, BB, size=1000, aspect=True):\n",
    "    ### resize image\n",
    "    if aspect:\n",
    "        if image.shape[1] > image.shape[0]:\n",
    "            size = (size, int(size*image.shape[0]/image.shape[1]))\n",
    "        else:\n",
    "            size = (int(size*image.shape[1]/image.shape[0]), size)\n",
    "    else:\n",
    "        size = (size, size)\n",
    "\n",
    "    img_resized = cv2.resize(image.copy(), size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    ### resize BB\n",
    "    # get x and y ratio\n",
    "    lx = size[0]/image.shape[1]\n",
    "    ly = size[1]/image.shape[0]\n",
    "    \n",
    "    # get new (x,y), width, height\n",
    "    BB_new = [int(BB[0]*lx), int(BB[1]*ly), int(BB[2]*lx), int(BB[3]*ly)]\n",
    "    \n",
    "    return img_resized, BB_new\n",
    "\n",
    "def edgeBoxDetection(image):\n",
    "    proposals = np.array([])\n",
    "    model = os.path.join(os.getcwd(), 'Project1.2', 'model.yml')\n",
    "    edge_detection = cv2.ximgproc.createStructuredEdgeDetection(model)\n",
    "    edges = edge_detection.detectEdges(np.float32(image) / 255.0)\n",
    "\n",
    "    orimap = edge_detection.computeOrientation(edges)\n",
    "    edges = edge_detection.edgesNms(edges, orimap)\n",
    "\n",
    "    edge_boxes = cv2.ximgproc.createEdgeBoxes()\n",
    "    edge_boxes.setMaxBoxes(2000)\n",
    "    boxes = edge_boxes.getBoundingBoxes(edges, orimap)\n",
    "\n",
    "    return boxes[0]\n",
    "\n",
    "def check_boarder_gap(w, h, w_max, h_max):\n",
    "    if (w > w_max) or (h > h_max):\n",
    "        return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image correspondance with annotations\n",
    "image_annot_correspondance = {}\n",
    "for image_id in range(1500):\n",
    "    image_annot_correspondance[str(image_id)] = []\n",
    "    for annotation_id in range(len(annotations['annotations'])):\n",
    "        if int(annotations['annotations'][annotation_id]['image_id']) == image_id:\n",
    "            image_annot_correspondance[str(image_id)].append(annotation_id+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = os.path.join(os.getcwd(), 'Project1.2', 'data', 'raw', 'train')\n",
    "size = 224\n",
    "iou_threshold = 0.5\n",
    "width_cap = 256\n",
    "height_cap = 256\n",
    "dim = [width_cap, height_cap]\n",
    "\n",
    "def parallel_test1(image_name):\n",
    "    dictionary ={\n",
    "    \"groundbox\" : [],\n",
    "    \"proposal\" : [],\n",
    "    \"annotation_id\": ''\n",
    "}\n",
    "    image_id = int(image_name.split('.')[0])\n",
    "    path = os.path.join(path_train, image_name)\n",
    "    image = cv2.imread(path)\n",
    "    BB_proposals = edgeBoxDetection(image)\n",
    "\n",
    "    true_images = []\n",
    "    false_images = []\n",
    "\n",
    "    counter = -1\n",
    "    for proposal in BB_proposals:\n",
    "        false_flag = False\n",
    "        counter += 1\n",
    "        best = None\n",
    "        for annotation in image_annot_correspondance[str(image_id)]:\n",
    "            BB = [\n",
    "                int(annotations['annotations'][annotation-1]['bbox'][0]),\n",
    "                int(annotations['annotations'][annotation-1]['bbox'][1]),\n",
    "                int(annotations['annotations'][annotation-1]['bbox'][0] + annotations['annotations'][annotation-1]['bbox'][2]), \n",
    "                int(annotations['annotations'][annotation-1]['bbox'][1] + annotations['annotations'][annotation-1]['bbox'][3])\n",
    "                ]\n",
    "            \n",
    "            if counter == 1:\n",
    "                try:\n",
    "                    gt = image[BB[1]:BB[3], BB[0]:BB[2]]\n",
    "                    if (abs(BB[2] - BB[0]) > width_cap) or (abs(BB[3] - BB[1]) > height_cap):\n",
    "                        gt = cv2.resize(gt, dim, interpolation = cv2.INTER_AREA)\n",
    "                    cv2.imwrite(os.path.join(path_train.replace('raw', 'splitted'), supercategories[str(annotations['annotations'][annotation-1]['category_id'])], f'{image_id}.jpg'), gt)\n",
    "\n",
    "                except:\n",
    "                    false_flag = True\n",
    "                    break\n",
    "\n",
    "            proposal = [proposal[0], proposal[1], proposal[0]+proposal[2], proposal[1]+proposal[3]]\n",
    "            iou = get_iou(BB, proposal)\n",
    "\n",
    "            if best == None:\n",
    "                    best = [image[proposal[1]:proposal[3], proposal[0]:proposal[2]], iou, annotations['annotations'][annotation-1]['category_id']]\n",
    "                    dictionary[\"groundbox\"] = annotations['annotations'][annotation-1]['bbox']\n",
    "                    dictionary[\"proposal\"] = [proposal[0], proposal[1], abs(proposal[2] - proposal[0]), abs(proposal[3] - proposal[1])]\n",
    "                    dictionary[\"annotation_id\"] = annotation-1\n",
    "            elif best[1] < iou:\n",
    "                    best = [image[proposal[1]:proposal[3], proposal[0]:proposal[2]], iou, annotations['annotations'][annotation-1]['category_id']]\n",
    "                    dictionary[\"groundbox\"] = annotations['annotations'][annotation-1]['bbox']\n",
    "                    dictionary[\"proposal\"] = [proposal[0], proposal[1], abs(proposal[2] - proposal[0]), abs(proposal[3] - proposal[1])]\n",
    "                    dictionary[\"annotation_id\"] = annotation-1\n",
    "\n",
    "        if not false_flag:\n",
    "\n",
    "            if best[1] >= 0.5:\n",
    "                true_images.append([best, dictionary])\n",
    "            elif best[1] < 0.3:\n",
    "                false_images.append([best, dictionary])\n",
    "\n",
    "    write_counter = 0\n",
    "    for true_id in range(len(true_images)):\n",
    "        img = true_images[true_id][0]\n",
    "        image_width = img.shape[0]\n",
    "        image_height = img.shape[1]\n",
    "        resize_flag = check_boarder_gap(image_width, image_height, width_cap, height_cap)\n",
    "        if resize_flag:\n",
    "            img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        name = os.path.join(path_train.replace('raw', 'splitted'), supercategories[str(true_images[1][true_id][2])], f'{image_id}_{write_counter}.jpg')\n",
    "        cv2.imwrite(name, img)\n",
    "        with open(name.replace('.jpg', '.json'), \"w\") as outfile:\n",
    "            json.dump(true_images, outfile)\n",
    "        write_counter += 1\n",
    "\n",
    "    false_num = int(3*len(true_images))\n",
    "    print(len(true_images), len(false_images))\n",
    "    for false_id in range(false_num):\n",
    "        if false_id > len(false_images) -1 : break\n",
    "        img = false_images[false_id][0]\n",
    "        image_width = img.shape[0]\n",
    "        image_height = img.shape[1]\n",
    "        resize_flag = check_boarder_gap(image_width, image_height, width_cap, height_cap)\n",
    "        if resize_flag:\n",
    "            img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        cv2.imwrite(os.path.join(path_train.replace('raw', 'splitted'), 'Background', f'{image_id}_{write_counter}.jpg'), img)\n",
    "        write_counter += 1\n",
    "\n",
    "Parallel(n_jobs=24)(delayed(parallel_test1)(image_name) for image_name in tqdm(os.listdir(path_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = os.path.join(os.getcwd(), 'Project1.2', 'data', 'raw', 'test')\n",
    "size = 224\n",
    "iou_threshold = 0.5\n",
    "width_cap = 256\n",
    "height_cap = 256\n",
    "dim = [width_cap, height_cap]\n",
    "\n",
    "def parallel_test(image_name):\n",
    "    image_id = int(image_name.split('.')[0])\n",
    "    \n",
    "    path = os.path.join(path_test, image_name)\n",
    "    image = cv2.imread(path)\n",
    "    BB_proposals = edgeBoxDetection(image)\n",
    "\n",
    "    true_images = []\n",
    "    false_images = []\n",
    "\n",
    "    counter = -1\n",
    "    for proposal in BB_proposals:\n",
    "        false_flag = False\n",
    "        counter += 1\n",
    "        proposed_images = []\n",
    "        best = None\n",
    "        for annotation in image_annot_correspondance[str(image_id)]:\n",
    "            BB = [\n",
    "                int(annotations['annotations'][annotation-1]['bbox'][0]),\n",
    "                int(annotations['annotations'][annotation-1]['bbox'][1]),\n",
    "                int(annotations['annotations'][annotation-1]['bbox'][0] + annotations['annotations'][annotation-1]['bbox'][2]), \n",
    "                int(annotations['annotations'][annotation-1]['bbox'][1] + annotations['annotations'][annotation-1]['bbox'][3])\n",
    "                ]\n",
    "            \n",
    "            if counter == 1:\n",
    "                try:\n",
    "                    gt = image[BB[1]:BB[3], BB[0]:BB[2]]\n",
    "                    if (abs(BB[2] - BB[0]) > width_cap) or (abs(BB[3] - BB[1]) > height_cap):\n",
    "                        gt = cv2.resize(gt, dim, interpolation = cv2.INTER_AREA)\n",
    "                    cv2.imwrite(os.path.join(path_test.replace('raw', 'splitted'), supercategories[str(annotations['annotations'][annotation-1]['category_id'])], f'{image_id}.jpg'), gt)\n",
    "                except:\n",
    "                    false_flag = True\n",
    "                    break\n",
    "\n",
    "            proposal = [proposal[0], proposal[1], proposal[0]+proposal[2], proposal[1]+proposal[3]]\n",
    "            iou = get_iou(BB, proposal)\n",
    "\n",
    "            \n",
    "            if best == None:\n",
    "                best = [image[proposal[1]:proposal[3], proposal[0]:proposal[2]], iou, annotations['annotations'][annotation-1]['category_id']]\n",
    "            elif best[1] < iou:\n",
    "                best = [image[proposal[1]:proposal[3], proposal[0]:proposal[2]], iou, annotations['annotations'][annotation-1]['category_id']]\n",
    "                \n",
    "        if not false_flag:\n",
    "            if best[1] >= 0.5:\n",
    "                true_images.append(best)\n",
    "            elif best[1] < 0.3:\n",
    "                false_images.append(best)\n",
    "\n",
    "    write_counter = 0\n",
    "    for true_id in range(len(true_images)):\n",
    "        img = true_images[true_id][0]\n",
    "        image_width = img.shape[0]\n",
    "        image_height = img.shape[1]\n",
    "        resize_flag = check_boarder_gap(image_width, image_height, width_cap, height_cap)\n",
    "        if resize_flag:\n",
    "            img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        cv2.imwrite(os.path.join(path_test.replace('raw', 'splitted'), supercategories[str(true_images[true_id][2])], f'{image_id}_{write_counter}.jpg'), img)\n",
    "        write_counter += 1\n",
    "\n",
    "    false_num = int(3*len(true_images))\n",
    "    print(len(true_images), len(false_images))\n",
    "    for false_id in range(false_num):\n",
    "        if false_id > len(false_images) -1 : break\n",
    "        if false_id > len(false_images) -1 : break\n",
    "        img = false_images[false_id][0]\n",
    "        image_width = img.shape[0]\n",
    "        image_height = img.shape[1]\n",
    "        resize_flag = check_boarder_gap(image_width, image_height, width_cap, height_cap)\n",
    "        if resize_flag:\n",
    "            img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        cv2.imwrite(os.path.join(path_test.replace('raw', 'splitted'), 'Background', f'{image_id}_{write_counter}.jpg'), img)\n",
    "        write_counter += 1\n",
    "\n",
    "Parallel(n_jobs=24)(delayed(parallel_test)(image_name) for image_name in tqdm(os.listdir(path_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in proposed_images:\n",
    "    if image[1] > 0.5:\n",
    "        plt.imshow(image[0])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taco class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taco(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, transform, data_path=os.path.join(os.getcwd(), 'Project1.2', 'data', 'splitted')):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        data_path = os.path.join(data_path, 'train' if train else 'test')\n",
    "        image_classes = [os.path.split(d)[1] for d in glob.glob(data_path +'/*') if os.path.isdir(d)]\n",
    "        image_classes.sort()\n",
    "        self.name_to_label = {c: id for id, c in enumerate(image_classes)}\n",
    "        self.image_paths = glob.glob(data_path + '/*/*.jpg')\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        c = os.path.split(os.path.split(image_path)[0])[1]\n",
    "        y = self.name_to_label[c]\n",
    "        X = self.transform(image)\n",
    "        return X, y\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trans = transforms.Compose([ \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Resize((256, 256)),\n",
    "                            ])\n",
    "\n",
    "trainset = Taco(train=True, transform=trans)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "testset = Taco(train=False, transform=trans)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "print(f\"Image shape: {images[0].numpy()[0].shape}\")\n",
    "\n",
    "for i in range(21):\n",
    "    plt.subplot(5,7,i+1)\n",
    "    plt.imshow(images[i].numpy()[0], 'gray')\n",
    "    plt.title(labels[i].item())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "annotations = json.load(open(os.path.join('Project1.2', 'annotations.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "data_path = '/dtu/datasets1/02514/data_wastedetection/'\n",
    "images = {}\n",
    "for image_id in range(len(annotations['images'])):\n",
    "    images[image_id] = cv2.imread(os.path.join(data_path, annotations['images'][image_id]['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bounding box for every image \n",
    "annotations['bounding_boxes'] = {}\n",
    "for image_id in range(len(annotations['images'])):\n",
    "    bbxstart = 10000\n",
    "    bbxfin = 0\n",
    "    bbystart = 10000\n",
    "    bbyfin = 0\n",
    "    for id, value in enumerate(annotations['annotations'][image_id]['segmentation'][0]):\n",
    "        if id % 2 == 0:\n",
    "            if bbxstart > value: bbxstart = value\n",
    "            if bbxfin < value: bbxfin = value\n",
    "        else:\n",
    "            if bbystart > value: bbystart = value\n",
    "            if bbyfin < value: bbyfin = value\n",
    "\n",
    "    annotations['bounding_boxes'][image_id] = [int(bbxstart), int(bbystart), int(bbxfin), int(bbyfin)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize images and bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resize images and bounding boxes\n",
    "# resized_images = []\n",
    "# annotations['new_bounding_boxes'] = {}\n",
    "# for image_id in images:\n",
    "    \n",
    "#     # Resize images and bounding boxes\n",
    "#     width = annotations['bounding_boxes'][image_id][2] - annotations['bounding_boxes'][image_id][0]\n",
    "#     height = annotations['bounding_boxes'][image_id][3] - annotations['bounding_boxes'][image_id][1]\n",
    "#     new_image, new_bb, new_width, new_height = resize(images[image_id], (annotations['bounding_boxes'][image_id][0], annotations['bounding_boxes'][image_id][1]), width, height)\n",
    "    \n",
    "#     # write resized images\n",
    "#     resized_images.append(new_image)\n",
    "\n",
    "#     # write new bounding boxes\n",
    "#     annotations['new_bounding_boxes'][image_id] = [int(new_bb[0]), int(new_bb[1]), int(new_bb[0] + new_width), int(new_bb[1] + new_height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
