{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import os\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "import copy\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change project directory and run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run once\n",
    "data_path='/dtu/datasets1/02514/data_wastedetection'\n",
    "project_path = os.path.join(os.getcwd(), 'Project1.2')\n",
    "os.mkdir(os.path.join(project_path, 'data'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'raw'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'splitted'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'raw', 'test'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'raw', 'train'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'splitted', 'test'))\n",
    "os.mkdir(os.path.join(project_path, 'data', 'splitted', 'train'))\n",
    "\n",
    "annotations = json.load(open(os.path.join('Project1.2/annotations.json')))\n",
    "supercategories = {}\n",
    "categories = ['Background']\n",
    "for i in range(len(annotations['categories'])):\n",
    "    supercategories[str(i)] = annotations['categories'][i]['supercategory']\n",
    "    if annotations['categories'][i]['supercategory'] not in categories:\n",
    "        categories.append(annotations['categories'][i]['supercategory'])\n",
    "        os.mkdir(os.path.join(project_path, 'data', 'raw', 'test', annotations['categories'][i]['supercategory']))\n",
    "        os.mkdir(os.path.join(project_path, 'data', 'raw', 'train', annotations['categories'][i]['supercategory']))\n",
    "        os.mkdir(os.path.join(project_path, 'data', 'splitted', 'test', annotations['categories'][i]['supercategory']))\n",
    "        os.mkdir(os.path.join(project_path, 'data', 'splitted', 'train', annotations['categories'][i]['supercategory']))\n",
    "\n",
    "\n",
    "for image_id in range(len(annotations['images'])):\n",
    "    if int(annotations['images'][image_id]['file_name'].split('_')[1].split('/')[0]) < 13:\n",
    "        shutil.copyfile(os.path.join(data_path, annotations['images'][image_id]['file_name']), os.path.join(project_path, 'data', 'raw', 'train', supercategories[str(annotations['annotations'][image_id]['category_id'])], annotations['images'][image_id]['file_name'].split('/')[1]))\n",
    "    else:\n",
    "        shutil.copyfile(os.path.join(data_path, annotations['images'][image_id]['file_name']), os.path.join(project_path, 'data', 'raw', 'test', supercategories[str(annotations['annotations'][image_id]['category_id'])], annotations['images'][image_id]['file_name'].split('/')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taco class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aluminium foil': 0, 'Battery': 1, 'Blister pack': 2, 'Bottle': 3, 'Bottle cap': 4, 'Broken glass': 5, 'Can': 6, 'Carton': 7, 'Cigarette': 8, 'Cup': 9, 'Food waste': 10, 'Glass jar': 11, 'Lid': 12, 'Other plastic': 13, 'Paper': 14, 'Paper bag': 15, 'Plastic bag & wrapper': 16, 'Plastic container': 17, 'Plastic glooves': 18, 'Plastic utensils': 19, 'Pop tab': 20, 'Rope & strings': 21, 'Scrap metal': 22, 'Shoe': 23, 'Squeezable tube': 24, 'Straw': 25, 'Styrofoam piece': 26, 'Unlabeled litter': 27}\n",
      "{'Aluminium foil': 0, 'Battery': 1, 'Blister pack': 2, 'Bottle': 3, 'Bottle cap': 4, 'Broken glass': 5, 'Can': 6, 'Carton': 7, 'Cigarette': 8, 'Cup': 9, 'Food waste': 10, 'Glass jar': 11, 'Lid': 12, 'Other plastic': 13, 'Paper': 14, 'Paper bag': 15, 'Plastic bag & wrapper': 16, 'Plastic container': 17, 'Plastic glooves': 18, 'Plastic utensils': 19, 'Pop tab': 20, 'Rope & strings': 21, 'Scrap metal': 22, 'Shoe': 23, 'Squeezable tube': 24, 'Straw': 25, 'Styrofoam piece': 26, 'Unlabeled litter': 27}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 4000, 6000] at entry 0 and [3, 4032, 3024] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/zhome/a9/e/164550/dimos/Deep-Learning-For-Computer-Vision/Project1.2/Project_1_2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin.hpc.dtu.dk/zhome/a9/e/164550/dimos/Deep-Learning-For-Computer-Vision/Project1.2/Project_1_2.ipynb#ch0000015vscode-remote?line=29'>30</a>\u001b[0m testset \u001b[39m=\u001b[39m Taco(train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, transform\u001b[39m=\u001b[39mtransforms\u001b[39m.\u001b[39mToTensor())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin.hpc.dtu.dk/zhome/a9/e/164550/dimos/Deep-Learning-For-Computer-Vision/Project1.2/Project_1_2.ipynb#ch0000015vscode-remote?line=30'>31</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(testset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin.hpc.dtu.dk/zhome/a9/e/164550/dimos/Deep-Learning-For-Computer-Vision/Project1.2/Project_1_2.ipynb#ch0000015vscode-remote?line=32'>33</a>\u001b[0m images, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_loader))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin.hpc.dtu.dk/zhome/a9/e/164550/dimos/Deep-Learning-For-Computer-Vision/Project1.2/Project_1_2.ipynb#ch0000015vscode-remote?line=33'>34</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin.hpc.dtu.dk/zhome/a9/e/164550/dimos/Deep-Learning-For-Computer-Vision/Project1.2/Project_1_2.ipynb#ch0000015vscode-remote?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mImage shape: \u001b[39m\u001b[39m{\u001b[39;00mimages[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1200'>1201</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1201'>1202</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1202'>1203</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1226'>1227</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1227'>1228</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1228'>1229</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1229'>1230</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/dimos/venv/lib/python3.8/site-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/_utils.py?line=429'>430</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/_utils.py?line=430'>431</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/_utils.py?line=431'>432</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/_utils.py?line=432'>433</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/_utils.py?line=433'>434</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/zhome/a9/e/164550/dimos/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 4000, 6000] at entry 0 and [3, 4032, 3024] at entry 1\n"
     ]
    }
   ],
   "source": [
    "class Taco(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, transform, data_path=os.path.join(os.getcwd(), 'Project1.2', 'data', 'raw')):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        data_path = os.path.join(data_path, 'train' if train else 'test')\n",
    "        image_classes = [os.path.split(d)[1] for d in glob.glob(data_path +'/*') if os.path.isdir(d)]\n",
    "        image_classes.sort()\n",
    "        self.name_to_label = {c: id for id, c in enumerate(image_classes)}\n",
    "        self.image_paths = glob.glob(data_path + '/*/*.jpg')\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        c = os.path.split(os.path.split(image_path)[0])[1]\n",
    "        y = self.name_to_label[c]\n",
    "        X = self.transform(image)\n",
    "        return X, y\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = Taco(train=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "testset = Taco(train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "print(f\"Image shape: {images[0].numpy()[0].shape}\")\n",
    "\n",
    "for i in range(21):\n",
    "    plt.subplot(5,7,i+1)\n",
    "    plt.imshow(images[i].numpy()[0], 'gray')\n",
    "    plt.title(labels[i].item())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "annotations = json.load(open(os.path.join('Project1.2', 'annotations.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "data_path = '/dtu/datasets1/02514/data_wastedetection/'\n",
    "images = {}\n",
    "for image_id in range(len(annotations['images'])):\n",
    "    images[image_id] = cv2.imread(os.path.join(data_path, annotations['images'][image_id]['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bounding box for every image \n",
    "annotations['bounding_boxes'] = {}\n",
    "for image_id in range(len(annotations['images'])):\n",
    "    bbxstart = 10000\n",
    "    bbxfin = 0\n",
    "    bbystart = 10000\n",
    "    bbyfin = 0\n",
    "    for id, value in enumerate(annotations['annotations'][image_id]['segmentation'][0]):\n",
    "        if id % 2 == 0:\n",
    "            if bbxstart > value: bbxstart = value\n",
    "            if bbxfin < value: bbxfin = value\n",
    "        else:\n",
    "            if bbystart > value: bbystart = value\n",
    "            if bbyfin < value: bbyfin = value\n",
    "\n",
    "    annotations['bounding_boxes'][image_id] = [int(bbxstart), int(bbystart), int(bbxfin), int(bbyfin)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize images and bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, BB_start, BB_width, BB_height, size=(224, 224)):\n",
    "    ### resize image\n",
    "    img_resized = cv2.resize(image.copy(), size, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    ### resize BB\n",
    "    # get x and y ratio\n",
    "    lx = size[0]/image.shape[1]\n",
    "    ly = size[1]/image.shape[0]\n",
    "    \n",
    "    # get new (x,y), width, height\n",
    "    BB_new_start = (int(BB_start[0]*lx), int(BB_start[1]*ly))\n",
    "    BB_new_width = int(BB_width*ly)\n",
    "    BB_new_height = int(BB_height*lx)\n",
    "    \n",
    "    return img_resized, BB_new_start, BB_new_width, BB_new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resize images and bounding boxes\n",
    "# resized_images = []\n",
    "# annotations['new_bounding_boxes'] = {}\n",
    "# for image_id in images:\n",
    "    \n",
    "#     # Resize images and bounding boxes\n",
    "#     width = annotations['bounding_boxes'][image_id][2] - annotations['bounding_boxes'][image_id][0]\n",
    "#     height = annotations['bounding_boxes'][image_id][3] - annotations['bounding_boxes'][image_id][1]\n",
    "#     new_image, new_bb, new_width, new_height = resize(images[image_id], (annotations['bounding_boxes'][image_id][0], annotations['bounding_boxes'][image_id][1]), width, height)\n",
    "    \n",
    "#     # write resized images\n",
    "#     resized_images.append(new_image)\n",
    "\n",
    "#     # write new bounding boxes\n",
    "#     annotations['new_bounding_boxes'][image_id] = [int(new_bb[0]), int(new_bb[1]), int(new_bb[0] + new_width), int(new_bb[1] + new_height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
