{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "\n",
    "# pip install torchsummary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from typing import List, Callable, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "from sklearn.externals._pilutil import bytescale\n",
    "from skimage.util import crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/dtu/datasets1/02514/isic/train_allstyles'\n",
    "train_style0 = '/dtu/datasets1/02514/isic/train_style0'\n",
    "train_style1 = '/dtu/datasets1/02514/isic/train_style1'\n",
    "train_style2 = '/dtu/datasets1/02514/isic/train_style2'\n",
    "test_style0 = '/dtu/datasets1/02514/isic/test_style0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will run on GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_depth(shape, max_depth=10, print_out=True):\n",
    "    shapes = []\n",
    "    shapes.append(shape)\n",
    "    for level in range(1, max_depth):\n",
    "        if shape % 2 ** level == 0 and shape / 2 ** level > 1:\n",
    "            shapes.append(shape / 2 ** level)\n",
    "            if print_out:\n",
    "                print(f'Level {level}: {shape / 2 ** level}')\n",
    "        else:\n",
    "            if print_out:\n",
    "                print(f'Max-level: {level - 1}')\n",
    "            break\n",
    "\n",
    "    return shapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ELEKTRONN/elektronn3/blob/master/elektronn3/models/unet.py\n",
    "\n",
    "@torch.jit.script\n",
    "def autocrop(encoder_layer: torch.Tensor, decoder_layer: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Center-crops the encoder_layer to the size of the decoder_layer,\n",
    "    so that merging (concatenation) between levels/blocks is possible.\n",
    "    This is only necessary for input sizes != 2**n for 'same' padding and always required for 'valid' padding.\n",
    "    \"\"\"\n",
    "    if encoder_layer.shape[2:] != decoder_layer.shape[2:]:\n",
    "        ds = encoder_layer.shape[2:]\n",
    "        es = decoder_layer.shape[2:]\n",
    "        assert ds[0] >= es[0]\n",
    "        assert ds[1] >= es[1]\n",
    "        if encoder_layer.dim() == 4:  # 2D\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2)\n",
    "                            ]\n",
    "        elif encoder_layer.dim() == 5:  # 3D\n",
    "            assert ds[2] >= es[2]\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2),\n",
    "                            ((ds[2] - es[2]) // 2):((ds[2] + es[2]) // 2),\n",
    "                            ]\n",
    "    return encoder_layer, decoder_layer\n",
    "\n",
    "\n",
    "def conv_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.Conv3d\n",
    "    elif dim == 2:\n",
    "        return nn.Conv2d\n",
    "\n",
    "\n",
    "def get_conv_layer(in_channels: int,\n",
    "                   out_channels: int,\n",
    "                   kernel_size: int = 3,\n",
    "                   stride: int = 1,\n",
    "                   padding: int = 1,\n",
    "                   bias: bool = True,\n",
    "                   dim: int = 2):\n",
    "    return conv_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                           bias=bias)\n",
    "\n",
    "\n",
    "def conv_transpose_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.ConvTranspose3d\n",
    "    elif dim == 2:\n",
    "        return nn.ConvTranspose2d\n",
    "\n",
    "\n",
    "def get_up_layer(in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 2,\n",
    "                 dim: int = 3,\n",
    "                 up_mode: str = 'transposed',\n",
    "                 ):\n",
    "    if up_mode == 'transposed':\n",
    "        return conv_transpose_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\n",
    "    else:\n",
    "        return nn.Upsample(scale_factor=2.0, mode=up_mode)\n",
    "\n",
    "\n",
    "def maxpool_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.MaxPool3d\n",
    "    elif dim == 2:\n",
    "        return nn.MaxPool2d\n",
    "\n",
    "\n",
    "def get_maxpool_layer(kernel_size: int = 2,\n",
    "                      stride: int = 2,\n",
    "                      padding: int = 0,\n",
    "                      dim: int = 2):\n",
    "    return maxpool_layer(dim=dim)(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "def get_activation(activation: str):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'leaky':\n",
    "        return nn.LeakyReLU(negative_slope=0.1)\n",
    "    elif activation == 'elu':\n",
    "        return nn.ELU()\n",
    "\n",
    "\n",
    "def get_normalization(normalization: str,\n",
    "                      num_channels: int,\n",
    "                      dim: int):\n",
    "    if normalization == 'batch':\n",
    "        if dim == 3:\n",
    "            return nn.BatchNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.BatchNorm2d(num_channels)\n",
    "    elif normalization == 'instance':\n",
    "        if dim == 3:\n",
    "            return nn.InstanceNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.InstanceNorm2d(num_channels)\n",
    "    elif 'group' in normalization:\n",
    "        num_groups = int(normalization.partition('group')[-1])  # get the group size from string\n",
    "        return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "\n",
    "\n",
    "class Concatenate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Concatenate, self).__init__()\n",
    "\n",
    "    def forward(self, layer_1, layer_2):\n",
    "        x = torch.cat((layer_1, layer_2), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 MaxPool.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 pooling: bool = True,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: str = 2,\n",
    "                 conv_mode: str = 'same'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "\n",
    "        # conv layers\n",
    "        self.conv1 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # pooling layer\n",
    "        if self.pooling:\n",
    "            self.pool = get_maxpool_layer(kernel_size=2, stride=2, padding=0, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # activation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "\n",
    "        before_pooling = y  # save the outputs before the pooling operation\n",
    "        if self.pooling:\n",
    "            y = self.pool(y)  # pooling\n",
    "        return y, before_pooling\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 UpConvolution/Upsample.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: int = 3,\n",
    "                 conv_mode: str = 'same',\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        # upconvolution/upsample layer\n",
    "        self.up = get_up_layer(self.in_channels, self.out_channels, kernel_size=2, stride=2, dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "        # conv layers\n",
    "        self.conv0 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv1 = get_conv_layer(2 * self.out_channels, self.out_channels, kernel_size=3, stride=1,\n",
    "                                    padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act0 = get_activation(self.activation)\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm0 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "        # concatenate layer\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "    def forward(self, encoder_layer, decoder_layer):\n",
    "        \"\"\" Forward pass\n",
    "        Arguments:\n",
    "            encoder_layer: Tensor from the encoder pathway\n",
    "            decoder_layer: Tensor from the decoder pathway (to be up'd)\n",
    "        \"\"\"\n",
    "        up_layer = self.up(decoder_layer)  # up-convolution/up-sampling\n",
    "        cropped_encoder_layer, dec_layer = autocrop(encoder_layer, up_layer)  # cropping\n",
    "\n",
    "        if self.up_mode != 'transposed':\n",
    "            # We need to reduce the channel dimension with a conv layer\n",
    "            up_layer = self.conv0(up_layer)  # convolution 0\n",
    "        up_layer = self.act0(up_layer)  # activation 0\n",
    "        if self.normalization:\n",
    "            up_layer = self.norm0(up_layer)  # normalization 0\n",
    "\n",
    "        merged_layer = self.concat(up_layer, cropped_encoder_layer)  # concatenation\n",
    "        y = self.conv1(merged_layer)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # acivation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "        return y\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 out_channels: int = 2,\n",
    "                 n_blocks: int = 4,\n",
    "                 start_filters: int = 32,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = 'batch',\n",
    "                 conv_mode: str = 'same',\n",
    "                 dim: int = 2,\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_blocks = n_blocks\n",
    "        self.start_filters = start_filters\n",
    "        self.activation = activation\n",
    "        self.normalization = normalization\n",
    "        self.conv_mode = conv_mode\n",
    "        self.dim = dim\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.down_blocks = []\n",
    "        self.up_blocks = []\n",
    "\n",
    "        # create encoder path\n",
    "        for i in range(self.n_blocks):\n",
    "            num_filters_in = self.in_channels if i == 0 else num_filters_out\n",
    "            num_filters_out = self.start_filters * (2 ** i)\n",
    "            pooling = True if i < self.n_blocks - 1 else False\n",
    "\n",
    "            down_block = DownBlock(in_channels=num_filters_in,\n",
    "                                   out_channels=num_filters_out,\n",
    "                                   pooling=pooling,\n",
    "                                   activation=self.activation,\n",
    "                                   normalization=self.normalization,\n",
    "                                   conv_mode=self.conv_mode,\n",
    "                                   dim=self.dim)\n",
    "\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # create decoder path (requires only n_blocks-1 blocks)\n",
    "        for i in range(n_blocks - 1):\n",
    "            num_filters_in = num_filters_out\n",
    "            num_filters_out = num_filters_in // 2\n",
    "\n",
    "            up_block = UpBlock(in_channels=num_filters_in,\n",
    "                               out_channels=num_filters_out,\n",
    "                               activation=self.activation,\n",
    "                               normalization=self.normalization,\n",
    "                               conv_mode=self.conv_mode,\n",
    "                               dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # final convolution\n",
    "        self.conv_final = get_conv_layer(num_filters_out, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                         bias=True, dim=self.dim)\n",
    "\n",
    "        # add the list of modules to current module\n",
    "        self.down_blocks = nn.ModuleList(self.down_blocks)\n",
    "        self.up_blocks = nn.ModuleList(self.up_blocks)\n",
    "\n",
    "        # initialize the weights\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.weight, **kwargs)  # weights\n",
    "\n",
    "    @staticmethod\n",
    "    def bias_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.bias, **kwargs)  # bias\n",
    "\n",
    "    def initialize_parameters(self,\n",
    "                              method_weights=nn.init.xavier_uniform_,\n",
    "                              method_bias=nn.init.zeros_,\n",
    "                              kwargs_weights={},\n",
    "                              kwargs_bias={}\n",
    "                              ):\n",
    "        for module in self.modules():\n",
    "            self.weight_init(module, method_weights, **kwargs_weights)  # initialize weights\n",
    "            self.bias_init(module, method_bias, **kwargs_bias)  # initialize bias\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        encoder_output = []\n",
    "\n",
    "        # Encoder pathway\n",
    "        for module in self.down_blocks:\n",
    "            x, before_pooling = module(x)\n",
    "            encoder_output.append(before_pooling)\n",
    "\n",
    "        # Decoder pathway\n",
    "        for i, module in enumerate(self.up_blocks):\n",
    "            before_pool = encoder_output[-(i + 2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        attributes = {attr_key: self.__dict__[attr_key] for attr_key in self.__dict__.keys() if '_' not in attr_key[0] and 'training' not in attr_key}\n",
    "        d = {self.__class__.__name__: attributes}\n",
    "        return f'{d}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISIC(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform_img, transform_label, train=False, test=False, data_path=data_path):\n",
    "        'Initialization'\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_label = transform_label\n",
    "\n",
    "        image_paths = os.listdir(os.path.join(data_path, 'Images'))\n",
    "        label_paths = os.listdir(os.path.join(data_path, 'Segmentations'))\n",
    "\n",
    "        print(f'Train: {train}, Test: {test}')\n",
    "        print(f'Size of all images: {len(image_paths) - 1}')\n",
    "        print(f'Size of all labels: {len(label_paths) - 1}')\n",
    "        \n",
    "        if train:\n",
    "            image_paths = image_paths[:int(len(image_paths)*0.8)]\n",
    "        else:\n",
    "            image_paths = image_paths[int(len(image_paths)*0.8):]\n",
    "\n",
    "        self.couples = []\n",
    "        for image in image_paths:\n",
    "            for label in label_paths:\n",
    "                if 'DS_' in image or 'DS_' in label: continue\n",
    "                if image[:12] in label:\n",
    "                    self.couples.append([os.path.join(data_path, 'Images', image), os.path.join(data_path, 'Segmentations', label)])\n",
    "            \n",
    "        print(len(self.couples))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.couples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.couples[idx][0]\n",
    "        label_path = self.couples[idx][1]\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        label = Image.open(label_path)\n",
    "\n",
    "        #image = np.moveaxis(image, -1, 0)  # from [H, W, C] to [C, H, W]\n",
    "        #label = np.moveaxis(image, -1, 0)  # from [H, W, C] to [C, H, W]\n",
    "\n",
    "        Y = self.transform_label(label)\n",
    "        X = self.transform_img(image)\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: True, Test: False\n",
      "Size of all images: 100\n",
      "Size of all labels: 300\n",
      "237\n",
      "Train: False, Test: False\n",
      "Size of all images: 100\n",
      "Size of all labels: 300\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "size = 256\n",
    "#normalize = transforms.Normalize(mean, std)\n",
    "train_transform_img = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                        std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "# Grey scale so normalization should be one channel\n",
    "train_transform_label = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5],\n",
    "                                                        std=[0.5])])\n",
    "\n",
    "validation_transform_img = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                        std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "validation_transform_label = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize(mean=[0.5],\n",
    "                                                        std=[0.5])])                                                    \n",
    "\n",
    "batch_size = 6\n",
    "trainset = ISIC(train=True, transform_img=train_transform_img, transform_label=train_transform_label)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "validationset = ISIC(train=False, transform_img=validation_transform_img, transform_label=validation_transform_label)\n",
    "validation_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "# batch_size = 6\n",
    "# trainset = ISIC(train=True, transform=train_transform)\n",
    "# train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "# validationset = ISIC(train=False, transform=validation_transform)\n",
    "# validation_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=3,\n",
    "             out_channels=1,\n",
    "             n_blocks=4,\n",
    "             start_filters=32,\n",
    "             activation='relu',\n",
    "             normalization='batch',\n",
    "             conv_mode='same',\n",
    "             dim=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1: 64.0\n",
      "Level 2: 32.0\n",
      "Level 3: 16.0\n",
      "Level 4: 8.0\n",
      "Level 5: 4.0\n",
      "Level 6: 2.0\n",
      "Max-level: 6\n"
     ]
    }
   ],
   "source": [
    "shape = 128\n",
    "out = compute_max_depth(shape, print_out=True, max_depth=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
    "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
    "                 epochs: int = 100,\n",
    "                 epoch: int = 0,\n",
    "                 notebook: bool = False\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.epoch = epoch\n",
    "        self.notebook = notebook\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.learning_rate = []\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "        self.model = self.model.to(self.device)\n",
    "        progressbar = trange(self.epochs, desc='Progress')\n",
    "        for i in progressbar:\n",
    "            \"\"\"Epoch counter\"\"\"\n",
    "            self.epoch += 1  # epoch counter\n",
    "\n",
    "            \"\"\"Training block\"\"\"\n",
    "            self._train()\n",
    "\n",
    "            \"\"\"Validation block\"\"\"\n",
    "            if self.validation_DataLoader is not None:\n",
    "                self._validate()\n",
    "\n",
    "            \"\"\"Learning rate scheduler block\"\"\"\n",
    "            if self.lr_scheduler is not None:\n",
    "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                    self.lr_scheduler.batch(self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
    "                else:\n",
    "                    self.lr_scheduler.batch()  # learning rate scheduler step\n",
    "        return self.training_loss, self.validation_loss, self.learning_rate\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.train()  # train mode\n",
    "        train_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            inp = x.to(self.device)\n",
    "            #inp = inp.type(torch.cuda.FloatTensor)\n",
    "            target = y.to(self.device)  # send to device (GPU or CPU)\n",
    "            #target = target.type(torch.cuda.FloatTensor)\n",
    "            self.optimizer.zero_grad()  # zerograd the parameters\n",
    "            out = self.model(inp)  # one forward pass\n",
    "            loss = self.criterion(target, out)  # calculate loss\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            loss.backward()  # one backward pass\n",
    "            self.optimizer.step()  # update the parameters\n",
    "\n",
    "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
    "\n",
    "        #train_l = train_losses.detach().cpu()\n",
    "        self.training_loss.append(np.mean(train_losses))\n",
    "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        batch_iter.close()\n",
    "\n",
    "    def _validate(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.eval()  # evaluation mode\n",
    "        valid_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            inp, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = self.model(inp)\n",
    "                loss = self.criterion(target, out)\n",
    "                loss_value = loss.item()\n",
    "                valid_losses.append(loss_value)\n",
    "\n",
    "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
    "            if i==0:\n",
    "                x_ = x\n",
    "                output_ = out\n",
    "        \n",
    "        y_ = torch.sigmoid(output_).detach().cpu()\n",
    "        for k in range(6):\n",
    "            plt.subplot(2, 6, k+1)\n",
    "            plt.imshow(np.rollaxis(x_[k].cpu().numpy(), 0, 3), cmap='gray')\n",
    "            plt.title('Real')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(2, 6, k+7)\n",
    "            plt.imshow(y_[k, 0], cmap='gray')\n",
    "            plt.title('Output')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle('%d / %d ' % (self.epoch, self.epochs))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        self.validation_loss.append(np.mean(valid_losses))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        batch_iter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "torch_devices = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "#device = torch_devices[-1]\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_real, y_pred):\n",
    "    return torch.mean(y_pred - y_real*y_pred + torch.log(1 + torch.exp(-y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ead5cd01cad4a45b33b54024d3745eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c432442cd1874bfca24a0d829d5e1107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADrCAYAAADKbEVrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6mUlEQVR4nO29e6xl2VnY+fvWWnvv87qvqlvv6ne33U3bYGyCHRKDmyROTHBsRWMkkjCQGYfMH4hohhA0I42UGaGQCCIBfwzOBMlAlCAmQMgEIUySGYNkO4HBD2y37W7a3V3dVV236lbd53nsvdda3/yxz61u2t23762ue6rq7PWTtu7jnLP3/p2z13fWXo9viaqSSCQSidlgbvcJJBKJRJtIQTeRSCRmSAq6iUQiMUNS0E0kEokZkoJuIpFIzJAUdBOJRGKGpKCbSCQSMyQF3UQikZghKegmbhki8kkRmYjI7nT72qse/1si8ryIDEXkt0Tk2Bvs78+LyKdf5zGd7mfvWL/4isdERP6ZiFybbv9MROTWWCYSb44UdBO3mh9R1cF0e+veP0XkceBfAD8AnAJGwP/xBvv668Dv7PP4t7ziWB99xf9/GPgw8C3ANwMfBP7+oU0SiSMgBd3ErPjbwH9Q1T9Q1V3gfwX+pogs7POa72H/oPt6/CDwz1X1RVW9CPxz4IduYj+JxC0nBd3EreanRGRdRD4lIu97xf8fB76w94eqPgNUwFteaycicoamRvy5fY71ByJyWUR+U0Tuf71jTX9//DASicRRkYJu4lbyE8CDwDng/wT+g4g8NH1sAGy96vlbwOvVdL8H+F19/YxM3wXcDzwKXAJ+W0Tc6xxrCxikdt3EnUAKuolbhqr+V1XdUdVSVX8Z+BRN8ATYBRZf9ZJFYOd1drdv08K0maJS1U3gHwAPAI+9zrEWgd19AngiMTNS0E0cJQrs1S6/TNOxBYCIPAgUwFOvfpGIZDQ12f94K441/f3Lh9hXInFkpKCbuCWIyLKI/FUR6YiIE5G/DXwn8LvTp/xr4IMi8l4R6QP/O/CbqvpaNd2/CPyJqm6/zrEeF5F3iIgVkQFNR9lF4CvTp/wK8D+JyDkROQv8GPBLt8o1kXgzuDd+SiJxIDLgJ2naWAPwVeDDqvoUgKp+WUT+B5rgexz4T8DffZ19vdFQsVPALwDngSHwaeB7VbWePv4vaNqWvzj9+xen/0skbjuSmrkSdxoi8iTw36jqk7f7XBKJW01qXkjcUYhIDvxKCriJeSXVdBOJRGKGpJpuIpFIzJAUdBOJRGKGpKCbSCQSMyQF3UQikZghKegmEonEDElBN5FIJGZICrqJRCIxQ1LQTSQSiRmSgm4ikUjMkBR0E4lEYoakoJtIJBIzJAXdRCKRmCEp6CYSicQMSUE3kUgkZkgKuolEIjFDUtBNJBKJGZKCbiKRSMyQFHQTiURihqSgm0gkEjMkBd1EIpGYISnoJhKJxAxJQTeRSCRmSAq6iUQiMUNS0E0kEokZkoJuIpFIzJA7PuiKyPtE5MXbfR5HTRs8k+P80AbPo3K8pUFXRJ4TkbGI7IrIZRH5JREZ3Mpj3Am0wTM5zg9t8LybHI+ipvtBVR0A7wC+Ffifj+AYdwJt8EyO80MbPO8KxyNrXlDVy8AnaN4AROQ9IvJpEdkUkS+IyPv2nisif1dEviIiOyLydRH5+0d1XreaNngmx/lwhHZ43vGOqnrLNuA54C9Pfz8PfBH4OeAccA34HppA/1emf5+YPvevAw8BAnwXMALeOX3sfcCLt/I8k2dybItjWzzvJsejEN8FdgAF/jOwDPwE8K9e9dxPAD/4Ovv5LeAf3Ikfbls8k+N8OLbF825yPIrmhQ+r6sL0hB8FVoH7gI9Mq/ebIrIJ/EXgDICIfEBE/ouIXJ8+9j3T193JtMEzOc6HI7TD865wdEe1Y1X9fRH5JeBngP9K823z9179PBEpgN8A/lvg36tqLSK/RVPdv+Npg2dyfJm72RHa4XmnOx71ON2fpWlD+TTwQRH5qyJiRaQjzRi480AOFMBVwIvIB4D3H/F53Wp+lvn3/FmS4zw4Qjs8f5Y71PFIg66qXgV+BfhR4EPA/0Ij+ALw44BR1Z3p4/8XsAH8LeD/PsrzutW0wTM5zocjtMPzTnaUaYNxIpFIJGbAHT8NOJFIJOaJFHQTiURihqSgm0gkEjMkBd1EIpGYIfuO0z23bDXLLcZaxMKgZ1k9vsTCwiJbO2OCDyyvLNNb6iPja/ROniHvLlB0BtgiR1E6vS6aO+y4JmyPkE6OZEodSzQY/O46483rLJ17FDGOEAIxRkQEUGo/IQSP9wGCMh5PqH2FsUq5tc3O1esMd7cJTsj7jsloF197JpPIl76qBxpv1wbPNjj+xE/8tK6urnLs2DH6/T6dTodOp0OWZTjnUFVibOoZ1lrA/5lzFBF8CBjjcARiPUZF0RAxMeIJYJrnWWMQY3DGYHa2ERSbOepOhxAtIhkSa1DFOYvECUSLRosVIRiHdPrUIeK9B+CJv/ItB7pe2+A5z477Bt2FFUuvn7Ow0MP1HHnM6UoXbwwLKwOGwwlFkZPnObGyZFlGnuc463BZhrGmGWVcBWKIZP0uIbdkErBlxWR7nZ3dklP3vY0s7xBCIIRAp9MBIARP9EKM4EQQPCaDrdGY0XjC7nBIPuixstwlBE9dVVB4xnFEnsc3vHjb5NkGx8FgwMLCAp1OhzzPKYrihocxhhgjIcQbv6s2+xUR/uwoHsEYg4rgY0BUiSFCDGSZwxgDASQqxkRAmwKvilpHZh2iQsBjnUGoieWQGIQiG6DicJkDaxHrsNZS1/WBHNviOc+O+wbd1eU+Io5MChwZZAWh6JF3LM45BstL9FwX18mYTBzOueZbA8gyx566U/CA7XWQ3GLLCdWwZFwKZ8/eh2rzxljrUGX6BgoaFcoaGyOqgfFwi9Fwl8loTLcYMDh9inH0eCJhd4T1UFcVGpUsO/hkuzZ4tsFxeXmJXq9LUeTTL5AMawzOGuq6bM5N7CsKphLjtHaEIKapOcUYmVQT8BVRIkTFxoj1EVGPWMUaQ1RFjIAICuj0OU6UGCq8DrG2gxhBtMaJw9mmMKsxTU3LGIyRQ82BaoPnPDvuezXbLKcYLOK6A4pOTvCB3a0Rp7pL9IpOI5YVZEVGcI7MCCYqee7IjEPR5g0AiixHAeOFclhSjsecvO/BRlAVa4umqm+VUI7xdU30NfiKajyirCq8qektLdJdXCCE5s3pBJj4Gs1yyhDo5F0KVxyq5tAGzzY4drqGrFDyDmSF4vKIEPHliPFoHWsWMPkSaFPIQvTEaoQVRSkIQZpaU2bRUCK+xBBQjfgQKTQSxUKWEaOiGhGboUaIxiJiIO+AVGi1STYaImEZ7SwgvSWMZATTIRiLEsmNR61gckPnEN0rbfCcZ8d9g+7WdsmZJdO0Y1hLv+hgAly/vku/36fIcygKjECWO0SmVfmo+LpubkuNIbOWGCPRe6gqtre3OHvvg9Dp4P302yl6NCq5UYbDLerJGBDUZnQWV+hbg48VZTmhqirEKEJzq2pCaNoNxZB1Fwi+Jlp/0Gu4FZ5tcCw6+Y2aUXPrKJRlCSGQd45hrMP7CCqgCnVAq5q6rjB5ICoYEWylxFBS+zGZMVjTOCsRI5bKV+CbW1sTDcYYXOaICmIMVVniyzFSDSEvIBa43gJIjuIgBgQwRoiGQ92VtcVznh33fcbyUp/t7TEnB4tYY8izjN6xZa6sr7O9M+TMoIdH2atPBx9wTskyi8sc1pqm8IqgAlYiG1deZHX1DOpyNDS3Ahoj1e4OVVU2t6R5j8HCMbIso6wm1L4GBakdRIcznkjNZDJBaN6wPM+JUZnECMZhbDjwB9wGzzY45nnTJm2txRhDCAGNkbzTRehS+x1UFWnaPZCoOBW8D4Qwgml9Pg4rgqmRQgljwFqMNQRpXqcxItrcggbvMXs1/2n7ItK8hxEPJiIWfDRkWY4xOSaUTMoJ1lswFiNC7Q9eSWiD5zw77ht0e4sLjIcjJtfXWVg9hWSKtY6TZ1cZTzw4ixMlxyG9PlIpJveEOCROKhxCPRzT6S8QnbB16SmUAqM1YXeHmDuiGgwZkndYGAzIshytA8EHvK/JpGmvQWAcGhmXZQRx4JXoa7LM4r3HOciyQF1XhHDwoNsGzzY45nl+oy2a0BTI3OUQIqpDJCqEiESIIUA1IcSK2nhkNMFVkdoAZSBIwARLPi7xkwmy1EEGC3j1ZFVTyFEQR1OIreDFkEfAZLjeMlEEU/QJJsNEQTRi8BAFhyKxglggqtgDX63t8Jxnx32Drg+RYysDLl+8ztbuGufvO40sL5BnOauDAcZarMnI1OG6C5SjIc5ajHTI+z06NsMMlhExVKMdsmyZk+fP4bI+Ihm18zjbAXXUVcZez2E5HiMihNAU1rKcNCfrHCJNzyIoWZYhIjfa/Ky1iDTtis4d/FamDZ5tcLTWTvdrIOqN2oqIoDS1mhg8fvqTakic7EK5S707ZLi1gxYZWo3BFrhOj1DuYoLFFY6oI5QuRHBicdY2vdUoRmwzssMEogaMsWT91WYoUoiobYKDj4pV0wQUqwTx+DBuetEPSBs859lx36u5aUuxnDm9xIXnt5o2kk6Bc2Y6Fs5gbY6JGTbvMRlvMVhcgGwR6XYwYrCq1JMxG2tXuPehxymtJ+AQcdjcgnGgFhMMwftmLFyWgza9iiGUWGuJITLtV2weyxwda5lMJoTY3HrUVUVdV8SohHDw27U2eLbB0VqLNXYasPVGsDfGNAVVFZUaRPGxJEyuETevU169wvqffp7rly6zeuY4G1cv0V88QTE4iVvosbJwBi8ZNq9QAekWGITKR8RZghVcCOikhCJDUYIqZF1yIrEcQd4ECpXpja8oGhWRSAwVIRw88VQbPOfZcd+ga0yGV0tvoc/DDzmGu5usnDiGwSEC1kZMrFDx4JrnirOY3BCCEvOc3O+yfuUlVs6eRTuOXISAA1eQ5+blJSycA1ViVLBdjK9QP6HIO6jCuBpiQonBEaYVeNWmNoV6MlG8r8ljTV3ZZhjHAWmDZxscC9dB1CA6DfDONoVSpxM0nBDjBA2BWE0Yb19n+NyX2PnaM3z+j/6QrWHJscKwNvKcWX6GEyt9Yr/HuQe+icXTb6W3cpbowHRyxIEg1DFgQiTWAa0DthCYjvKIYUR0Bd52MRopjEHKEiM1ZDnGdbC2oK5HaDx4JaENnvPsuG/Q3dgccvzYImIcvZVjjK9dZ/vqZU6dO4+JEeqIyTKyzGGN0O108d5TdBRHxGhNOdwihsDS8jKSZQgGazOwGcY23Ta+rlGYDiz2qHKjh9y4Gqc1mYnEaJG823wYJt6YgWKCkuGpGTWN2CVwiI60Nni2wdFZxUjTm6xS4UMzWN64TvMFACgZsZpQrq9RX1rjypN/xBc+9SRfGQauAivjyDZw+UrF6fWKl9jgL+0MyS5f4b5veg898wBSLGBdjWSW0FFUx3gRjDNM8LjMEjVAbM7buQwVmVbsm1q+amw6GmPz5RTjwSfztMFznh33DbrHVxbZ2Snp9XpUkrG6eozNK1cYb11nsNBlMp6QLw+w1gGRvOiws7ND3l/ChAqNno31K5w5ew8iBtWIMYKzlmgMMXgEJcTQTJ9TxYg0g/GjxWSWyWiD6CtyMcTuApL3mt7GMKauq6aNxwfq8Q5XL19iPHIY20WKg482b4NnGxxVaowTEIUQmkHzYjAIMQZCiMSYIZUn7m5w7U//mC9+9ms8Pww8CwyBdUCnP5+M0AF+/5lNzHNb/NVqzImda3Q3HkT7S+Srx8hkAVMpdjpDrxlgr0RVnCplVdLpFIi1aFCC9+SdpvYeQiDWfjqN9eBdaW3wnGfH/dt0neHUiUXQgEgg63Q4c985Lr90hd7CvSwvnQOTARZjMvLOMvXGGlpGjOtAtYHrDJC828yVrkpillE4xanHR0EJECuMNLNBokScqdHcUEsGE4FqE8l6kC0TtSbUNUYjJnoyrbm2ucbaxWtsbwknTi2Q2YLSbx/ow22LZyscMQhCFQNGBCMOIYcghKpCY4VTR41nHHZYv/oif3CtYkSzjCw0s+0A9urWE+CPIyxGRT7/NPc8f4nTq6ssrZ7kkb/4fhaycxTFEiaCjwqVb3rVCaAQjDbthOqIsURMpBJLphkmWmqNiExrTwe9XlvgOc+O+wbdtYsv0isKijyDoaGzegL6C5w9fYxLz1/goYc6aD5NOuEcopHcueZNMoErV9ZYOXN/M9B4L/GJmOn4OqbfXjTT+zoZglAygToQjSDGInkHyQrUOqzWTCYlguBV2L5ymfXnX8RLxunzJzjzoCF6pZoIoZoc6MNti2cbHKk8Yi22uftrZtlFuXE76usSGW1Sb20w3tzi2YtXGSlsvsFuI81zvlArX7myy/KVXc71LmLChMe+/a8hJ+/FWsVo0+anGYRQkhtLMehjrEWnY5gN4KIgBoKz0zsLIcvygzm2xXOOHfcNuksLfXq9XnMLGQLX1zaYlGusHOtwYrnPxee+zpmHHyUEj9Mc6yyLS0vU5QRjLC4v6HQWEBMRUaxzBHQ6DlMQsU3Hi7VYbXq1oekRNMYgRsg6A2K9QOkDOt7GBOXy5XWGG5toqDh7zwqyeJyqrIl1zcR6iLEZvnRA2uDZBkcTIjqpkMwQCGg0oEIMQgye6GvC1hVG6+vUmyU7Y3OjVnQQxtNtC7gyqln6k69y8vyDnOwMEBsxvo9bykFNM0uKpt0v1DXWWWpfk6sidYRckMIhJkdEcPbgQxzb4DnPjvs+2llZouh0KYoCZy1FkRO8Z7izxbX1bWSyy7Of+2Me/dZ3QN7FO4NzfeLumN3dLdyxY1Q2kluLWksZpg3S2lT4I9NOFzXUk7IZxhQhEKYp16CuPUQlbF7i+uUrbIwNC6vHufehe0GEsqqQTo8oFcF5KCe4jkf94MAfQBs82+BoTGwS9MSmU6+sdhGZZqHynjgZE0uPqqXavs5aVXGwvvRvxAPP7lZ8+f/7NAE4/fifp9fpQHkNMQOMsYjWMB7jgkEHFtstsC7DT5tfrATE+Omkp4N3pLXBc54d9w26zrobqdTEGFyWY7Mc11+ge09GtTPm4pNP8Yef+Rxv/5aHWDh+lmAtNYHNtXXOnzqJ+ICGiJqIlSYJhfc1xkdiLAEQ61AsGAPojcxBMQYUw9bmDldeWKc/GPDWh88T8gX8NL1qnvtmLF2IqCpZXiDGEg7VGzz/nm1w1LzJs+o1EDWCKiHUeO/x3jfz43sZfmvMc889yaWNg7f7vxoPPKOQf/0KJv4+veUl8uyteAaIOFynj+12QQo8FtHYNNvY5v1X1T+TkUo5+DjdNnjOs+MbTI7I2MtN2aQtawbSRwJiuyycPss3nTnDcO1FnvnsZxn0n6e31CfvZkhl0fGYMtZ4MRRF084RvCeEiGdENdnEitDp9aEzaAqsKsZME0jUJevPPQXOcM/jjzfTVBdXGWuGi3HaoVMR/YROt4t1GbEsCWEvcfbBaINnGxzJ3DS3quK0maBRTxPoWGup6ho/GrFz6Vm+9tQF1vzBA92rUaAGLig8f2GD/mc/TX95kW5+H1QBzSLR5BjXBWnauJsvn4iIaZLJT2dXNaX1EOfSBs85dtw36O5lYLfOodZQEbFi6NgcYzNcniMSWFhd5S3f9g6ufv0Fup0+Lz79DM7l1OWQwfIyvUGfsjDo7i7Be2zRw1jL5qWnkBgpsi69s2dwq/eQZRmVwNb1dYbb26ze9whZ4ZhUQyY7mzgxWGlqUd5HsjynMm6avKLCiCHGyaHGPbbBsw2Or6QKARFDjiGookGJtSdWJddf/Dpf37n529FXMgKeCxC//BzH7/si5/MurruC2kihzRjOIEpWNykvMZZgHc4KRmIz4uNG0szD0wbPeXPcv3lhuiyGasSaJt1fk4TC7U3ixJFh8j4yWOH02SEXnrmIV8eZhx5AN6+x+ZWvcrkqqWJksjlGvXLsVE7eM6x9bciJZaW7aPDb6wweriltj/WNTRbPnOfMI+/EaQStqMKEor9EICcah4bAtDkFayx++sHsfes04+UORhs82+C491zvm3xpzYj15jHVZsWAejxkZ3OT6/Hma0avpAK+BjCuufj0kyydP0/35L1E36eqapwpUJnmIp7mkjBx2vanNWqzZtzzIc6nDZ7z7PgGCW8CnaIAmvY7YwxZljVZf2SvIm2BDJv30aVVtH6Gyy9cZ2VxCX/1EtsvXGf1lGFnPfDiOlwBHt/yBOD6CEYTOLYbGT9/lRO7kbqzyCPv/FYWTpwFm6GxxntFjCXv9wiSIWLBKL6umo7FGKZthk2NqLmVLg78ZrfBsw2Odprrt8kFLGAEnCP4JomOMRCrEeV4F7k15RRobk0vKay9uMnJqxc4/sA7sdOZeGh8Rb6AZtaSkQCxpqqGaL5AVnQ4TPNCGzzn2XHfoButQNbMuGhynEYmZUnHFHStw4SK4ITgCvzGFS49/RU2t7d54KEm0YRf22C4pWwOA1I3DdY58NURbNP8vTKEB0vYjUr1wgbf8t6ThCxrBhprSdTIuA7k+QDnmv/XqpisQLGU5QQNZZMdXiPGTMfxHaKNpw2ebXCsgqChZLKzQZY5gsnQwuHVN2OLtSTEmnGIHHwU98HYAL6wWbH0padYved5TnWX8ZnF1Qp5n8w08/hVHTYz1L5CTcCa2Nya6sGbUNrgOc+O+2YSUW2ahs204yXLsiZD1HRQfF3VSBTGV77Oxa9/kW6/z9LKMmfPn+VEX5iUkUsRnqrgsjbT6EfASzQFVYDLwNc8GIWTVWDz+ReJm1eotzcJpScqdDodjHUIMk3xFqiqeroW13S6HkpVVa9agfZgtMGzDY7GWIKOmNSXqcoxoE0mKWNg2nzhih4rKwscP3g+pIO9v8CfBvidL17mj//jv2fz6U8RRtdAhDxWoCWKBwmoWozkuKyLyTuQ5USTHfhYbfCcZ8f9k5j3etMe6Kane68nOU7bBvE1G099nWtXnmP53gdYWVgmDEtc0aPcrqhGzRzoXZopePl0U5qa0YQm6g+BHYWNLbBru4xffAZrO2i2gOnm1KFujmss0Ux7DkMA2ywEpzFS137a2zlNC3eINt02eLbB0VpD4RYxC/dhRPDiaEYbNRnMrHPYTp+llRV6mYHy8B10++GBFyP83hefJ0z+Pe8/eY7uyr1QDZsavRVcbojRNMt3kxPUYU2BuIOfSxs859nxDb4jHHnew5iMZo15Q6c7QPIcrYesP/8Vrl66wIl7HubYqfMwLul2e8RY46wi8eWDXKeZ/TEGlmgKbJz+dEAP2J7AHz8XWfvKS5SbV6kn20hZY6NQlhXVaIyGQC4WZ5W6HFGXY2IUsqxDnndvjOM7zMoR7fCcf0cNJWAQ28drTvCCZbounDSdHzY30Fs5VNLww/J8pXz2hS12r61R1TUx+qZHezrMSKlQMZh8AaMGrQPuMAlvWuA5z45vOCev6fFm2tvdLE64tX6F4cZVLMrZt7yFpeNnMFnOtr9Cb3GB4CeUJnAtQpdmrnOkKaSe5n+BpnCOaXoNF4BFYNnD9o7n6kuXObmwhAOicxAjkjXJtINGqrJkPB7T7XZvrCyg+nKy48OsBtwWz3l3DN5Pc0A0eSH22oSNaebFB5q13QbdguN9h4z9YUbHHorLk8D4+homVkAO0iSKd86RTcegIoboPWr0MEnGWuE5z477fkUYhMxlN2YVVZOStYsXmWyv47KM5VPnWTh9DnUZRjKGVUnv2BK212f53GkeOW3oAAOa89gbmrQLLFihoinAi7x8m5plUFjFhSEvfOXLrF14nliOKFxzDt576qomRiXP82bBule829m00O7s7Bz4TW2DZxsc98bzipEbCxoKIKrT0RkG7R3HLp7kzz14kqWbGxp7ICofuX7xeXSyS9RpQvdpW+TeSst7KyE0PeIHpw2e8+y4f7289MTKE2vP7uY11p5/jj6GTJq2tsWlY1hpGo2raoRVQd2AYA21ywk1nM7gPE1hFZrCaIFxUGqaAjpm2hljYWEAuVWGV4eEkeKris31a1gPItNE164gy3JETDOfn+kwvhCIoy02rl4+XNamNni2wNEWGbbIMJmbrltlMUbIvScP4CTHFR2WTpzg2Olj2KO7K+W4wM7mOlqtE7REOl1cJ2cvF8peLc5kDpwBe/Co0QbPeXbct3lhWO9SVCMm27vULuPc6dMMt64hCKfPnKUqA3U9bIYgBd8MChaDEUO3KJCeYXujWUsIvTE1mYIm2hua2tHe3zsBejVYI6wcW+Lkw4+RnbwPiRXbG1cw/R55f6m59fQTjK8woaYc7yAibK6vM9rZZHEwgLx34De1DZ5tcMxcF6XpmBOz1xbcLBmPs0i02CgUgyWWTt3DQ4OvsLVV3pLZTK/mVEc4df4MmvfxRUFmcpzpYMzeJJVm3Tgxcug2yTZ4zrPjvs/YefElrr50ke5Sj9Xjp7h+bYOd7S1OnDhDE69ds3yFKnVVNdM3VXEuI3eO/sCgGYiDVZr2P0sz1GhI0xZY0gxINsCSg+PLhtX7T3L+mx6nOH4KZ8F1B/RO3kc9GbFx5RLUI/ATxJdQjqCeUO5u4sshKysrdKc99QelDZ5tcLSmg5UOQk4MkehDU1hzC7lFckfmLK7T59i9D/OB73iYkwcfqXVgOsCJs8ucePBbkP4KtjdohhSZApHsxmyrG7fMcKghjm3wnGfHfYOu7fVZPXOCEMasPfenbK5d48ypB6iNIwiIiURxWGOR4JlMKoxkuLzPzqhiY2J58JzlsXOGjmkKaaC5La1phhm9cnDF0nFh9S2nOPnw2ykWzxJwxE5BLCy1iRT9RTKJXLnwNGVVotC0U0YlRGVp9Tix6BPEHWo14DZ4tsHRmGzaBhgxlSfzEVsHggAI+IixrknO3l/mxEOP8Hg/41aX1fsy+HPveSe9c49AkVPYAgyojWAiKjVKhbEBo6A+IoeYytoGz3l23DfojmJNPZqw8dwau9d3OXPPKSa5IRqh9jVlVTZtGXsNybmhGg+58OSzTDTjm//82zn/tofpnllmkDVtga8+qEz/Pg70nZAtrGIHC2jmsEWBzTLyoqDT6SB5j6y/SK+/wPbaOt57tn3J2CvdxRXUdVAMcToe9aC0wbMNjjEqTDvogtDUhrrFjU6P3GVY53DOMlhY4sQ938R3vOsxHl8sbjLdzDeSAw8uOE6cWCXvdHHWEUNkb9nwGJVQg0YL6ppcrXK4RDBt8Jxnx33bdLvjyOZ4SLEw4P5HVgkIUtVIn72uO6rJGB8qhjs7XH1hjevbngfuP0ln0GFYe/LxWTpkPHR6i8mFwJZyY9peswRi0+M9EAiqlG5MIU1y7CCGWFfYzBERcAW2UHrOoXnGCxdeYKE/oL98klqg1kgIddNOaA7+1rfBsw2Oqk2BiCjRmmYVVwGJgiJEjThjwWVI0UUXzvLgt383H8668P/+EV8q45tuE7zPwiOPnmJw9mGK7qDJW2zNtCDKdHLK9I5immENDrcacBs859lx36D7ta9e4IFvOk/nWJ8QBGsUfEm5tUmMUFY1O7vb+MkEqSYsDBa5/22PInmOGKXQEZqNcC5nMxqW8sBK+fLA+r3hbI6moGrfkff7ZHmO0QC1ZxIDrujhigWsGsq6BrVgO/T6PTSUVPWQgEBVEquKEDzxEFkw2uDZBkfVJkgbcmysCZOrhMKiEWrvMTajwDXJdFyGyUoGq2c59dCj/LXtbTp/+BU+Py2pNzOffxF4xz0D3v7e76Szsgr9ZazJQWsIHUxsCmtUT7NAaFM7ilEP1QfRBs95dtx/GnARGQ9LwovXEbNFljUNxoN+RlZ0WRj0WDh3ktwE6t1dRjseRTChyfK+u3GF0fUrFGGH5ePCIEB2FV7yUE3PywAIDJaFU/efpb/8CNE4ggZcntHNm3XuQ2xSqimKoIxHuyysHEcVrm9cpygKfFVRlSUepQwHnzTQBs82OMLLuSV8FaAaEUuPiKVT9PEa8ZPmJEUMnU6BmuOcevjtYLs8ETz3PPsCuzslXxgru8AOB8uL5YDHHDz4wDlW73mcXv8YJiuw1iEoUFPXinMOEYtIk15zL+PbYWmD57w67ht0T913gsGp8wyKDNPNcJnDOoexzWyMGCNWMmKsIKsJpqauSjauXWayfZHO4hKLZx/E1EMe6Bguls/iuoEzHsYjJeSOURlYPdPlxOlVemfvxXR7BOsRY5Aso+j2qCL4umqyu1nLcHtEpyjQrMekjqwcM1y9ukYzmqRZNK6uqgO8ve3xbIOjiEGEJomOH2HqCS54IoKKELMO1jQ9zkaaVVxjp4+NhuPnagZPfA/n3vKnXL74dc5dusowwldf2uDLO4E3OoNjwDc/dJy3/4W/RHf5HmyxgHHZjZlLOu10CTGg0TWdRLgmv7EcYjpaSzzn2XHfoGs7XYajIZ3BSXKXE8VgxSEhgCpODC7LpglMSsYb13ihnrCwtMjqvW/FZBk+RJxE8tWHOPOtC9TVLlQTdq5v01s5RrQdFk+chN4iWVEQqaAWsjwHL0QRrDWor4kEQjVGyxIdLKIYrFFCNPQXV7i6doWeEYwq7hAfcBs82+Ao4gBPpEJMgMGAWI0JVQ0asQqSOfABQaidBSPkWZ+scxq3sEBx4jxLj307Z69cJg63ue+ZP2bpD5/iM7uRij9bUxIgm/58x5LjLe94jIWzb8V2+9AdICYHpjUhbdapizESQkkIFh8LjBUC0zW2DkgbPOfZcd+ga4wwHA3xwWN8k3rPOYfQfAPZvGDiI+P1lxheuchoIjzy2HlUhCqC9bH5FpIBdBfo37PMaHgdjZET9zsiOa67iCt6IDVuOt4thqaK7rKMEAGJxBiIdcXO9jZZ7qjqSbMGfYxUVYX3gcFgwO7VK0juDnUr0wbPNjiqAtLclubdJYJYxBSInaC2QG2OMRkOIZYVmTUYa5p2Y7tIz3TQXqBQy+LSWcqt63SWTrCUDeh/9gtc3qhZ803CnwnwzR24ryP4POMvvOsxHnz3Xyc/fgpT9LGui3MvZ0jby+4mIs0ac85Ne+KbVIWHmQbcBs95dty/pmsMRN8kHJEmF6r3NZmzxLrm8tV16t1tji8vcs+D9/Pc1y+BK1BVtje26A8GdLtdrLNoR/DisRXYYgEpBmS2yT1p8k6zwqa1lJMSlxeUZUXtldzRJD2pKnbXr5FlGT6W4EtC3QwpiVhCCIQQcVnG9miIyd8wl0+rPNvguDczKOJQ6YMzWJMjLieKpcY2nRwWyFxT6w6R6CNqHbZw2EKIEtHM0ytWsMeW6dmMzkrB9vUrXLxwjYtXdnlx5Pmubz7Fqfvu5cS5t3DswcfpnbmPWPSxnQUK1yGKv5HrIoaANRZrLEaaVWR9iNM8wu6Qoxfm33OeHfe9moM1LOQZk9Euxi0wGW6SGcPmcELc2mHp9ArHHrofp816ViqXCGXF+to6VeVZWlzC5R1MFhGBTHPoLyGuQ9ZfApPjQ2iSV4shhojNHRah08mp65rJcILimYy38bHESYHBMZ4M0RibBBTi8ZMJqGLyDB15dre2DnoNt8KzFY6hRsRiTIY1QnQexIA1aAjgPa5SYlCMc9TOUcSI2R0T8oDJu2jewYriQ9MW3XM5Vu/hvuPvw4xHvOUdQ66tX6PyJecffTsMVikWViiWjmE6fZwtsFmByRw6nUVlRDB1BRhMp4fY5kbWopioWAQjB78za4PnPDvuG3RL6ZB1AjvXruO0Ymt9ExdrTt93jsVHH0CzHsZZTIxsb++wdn2LYnODU2dOkxUFJu9BnqMOsjwjxkAolUotRhyZdeTTrBF1Xd5Isq1huvKm98RJjUhgtLVNv9+nrusm6fZ04LRqpNy9zng8bk7a5sSqZGN990Afbls82+CoGgmhmc3kXAbiiaqEaUAXEcgMakCnXw6egPQKrI2QO6I0WSSsyxBtFh3sLC4j/T6+BPGBwUMG0+siRbPOmytATEGRD24kaJFp+3UzpkMp8i626DZ3A9Na0l6Pd13Vh5oG3AbPeXbcv3lhvMtwssn2xW1GG5ucvuck/eMnsIvL1N0ORR3Y2thitL1D3zlOn13m2KmTZEW3mZ2R50jmmgHO4jBZhhgwYpGsYK/1w9f+Rmo07z2+9E2uVe9xCuPhLiZEokYUmhrR9Da1mozZWbvIcLckqkGzHDWRQecQc9lb4NkGx700e8YYVCNihFhH9gay2zwneE+oI2rAYpBuju0UeD9BbY61HVxdYkzRTPP0npj3iAjdTpMJLSs6+GKAzSzWCKVVbHRYMozdO7bDiJuO2VQ805lMQlMbmrYL7iVMOUzzQhs859lx36D74rMXiaHixIJFi5xuv4+vFR2WDHdGXBnusrDY4czZk2TWsv5izWRnRFEMMEUXjDQp/IxSa0VuM7A5oor3E6JphoVggVopR2OiRqyBEA2IxYeKzasvsXDyRLOibAiU1QQ/GrJ1bZ3h7ggziRRZF9vtUBe2GUlXH3zIWBs82+BYVRXdbhfgRgEQEVzWwYdA7SPOFjjJAEVMRKQgRouhIASHqAWzMx0W5JC6ovBKyAzGCtHmmO6Abm8JkWbKdEccxkWgWXMuMkEpMLYzbf8zQMCoa+bl2yaoNPmDm0H2h5kc0QbPeXbcN+i6bMzi6T6m6DNaHzLZ3WFntMUGgeP3nuL4QyfoxIIohlqErDeg2rxK3XGoPYHZy74znSIXY7yxicj01lPxdY2pAqEsUQ2Mym2EjDwfMCkDXoWqVsLmDju7LzGaXIdo6RQF5aRkWHt63ZxSIzGAGEPtD75cTxs82+BYVRV5nk97mfVGE4dYi5hmAU60ycsagkfV41wHpBljGaNnUo2wkpPnDgOUwwl1OSIr+pBnOFtA5hATsVZweYahg07TuOs0yfbLWdqa29O9+fh7t557SxHdmO56iOWl2uA5z477Bt2i22OyEymvbiERLr1wjfMPnKW7eoLeYAEbMqI0szLqGLFFl/Vnn6LX7yCDYxhrm/YXVWrv8SHcGMNmMMTa4+saX9WYyhN9IEZPNdoicx2qquL6i5cJ42tsbVyis7RMp5eRdY8z3B6xs72DMYblk4tUEYJXNCoyfcMOShs82+AYY5guaNnka3DOocq0wDZLFaEFRgxGIhGPTPNVBh8QCTgbm6mnklFXE4wxuKJZogXnMFlOtI4QK4zLKPIMjQaRDNXpLD4LddUkbcmyZphRqGu8KjZGxChZ1tTQlBpjmtcclDZ4zrPjvkF3azeQOaG/2KW71CWOS8gHoJaq8hRZhjPSNHrHgOlY6mKBbHEVpm0cdV2jsUlQURQFXjwxNsNBjFUm4zGEQB0CEj0SawyR7c2rbG9u4nd3OXHuDN3eCuRdyt0NNq9ex2WO3mKfSTmhqpqOzSKHMnqqqjrU2mFt8GyDo6eiijUmZljTTPow05qPsa4pbFEhmuZW0BhcnjXD07DN63xGNAY/GSLlCCPNyA/qQG4cJu/hXQdnZVoLM2BD0wzilaLIMaYgxOpGex8AWYZ6P30vm5qYCBgcgsEeIuq2wXOeHffPMna8R5ZluDzHOke20GVz4wo2d6gRjM/AN7eYWZYh1tDvd6nqSB4j4/GYLMuw1jaFWQWcEnxNFCHEiMaAE2EyGTPZWkfLIcEYik6H7tIKurSC7y0ScGxfvkRVlSwdP9H0FNY1wUeii9M2FUVpxoIepmOiDZ5tcJxMxjhXYCSnY910enNAqW4MZlciIfhmZpFpzlusAWOQEDHRYUSIwRANmLxLFE+eF5hunzAdm7nXxqhRwf3Zdry92pmo3OhgeSXGyA0nay1Zlh3qzqwNnvPsuP+MNO/Is4IsOsQbYreDrXbZ3tpiwVkobdMhEuN0fGak3ytYW7vGyU63OYHYFCJfe8g8kgtVXaGAcRYNgfWXLjO88DSd5eMMTt5DljUL0402Nuh2OtTjIdevrdMtMrqLS3hTUNUllYdgcmJWEwjT/JbT4/mDJ3Zrg2cbHMuyJMtKrHSa2kbkRhpAaGYSxahEbZYRUmnapo0I0TSJU0Qd6iPGWmxWEDBN4c8yfCdDA5hpL/aNwqovtznG6ftnjEEi07Y+JUZ/47G9dvC9NsHDrlzdBs95dpTDfMMmEolE4s1xhGtoJhKJROLVpKCbSCQSMyQF3UQikZghKegmEonEDElBN5FIJGZICrqJRCIxQ1LQTSQSiRmSgm4ikUjMkBR0E4lEYoakoJtIJBIzJAXdRCKRmCEp6CYSicQMSUE3kUgkZkgKuolEIjFDUtBNJBKJGZKCbiKRSMyQFHQTiURihqSgm0gkEjMkBd1EIpGYISnoJhKJxAxJQTeRSCRmSAq6iUQiMUNS0E0kEokZkoJuIpFIzJAUdBOJRGKGpKCbSCQSMyQF3UQikZghKegmEonEDElBN5FIJGZICrqJRCIxQ1LQTSQSiRmSgm4ikUjMkBR0E4lEYoakoJtIJBIzJAXdRCKRmCEp6CYSicQMSUE3kUgkZkgKuolEIjFDUtBNJBKJGZKCbiKRSMyQFHQTiURihqSgm0gkEjMkBd1EIpGYISnoJhKJxAxJQTeRSCRmSAq6iUQiMUNS0E0kEokZkoJuIpFIzJAUdBOJRGKGpKCbSCQSMyQF3UQikZghKegmEonEDElBN5FIJGZICrqJRCIxQ1LQTSQSiRlyU0FXRH5IRL4oIiMRuSwivyAiywd87XMi8pdv5riz2N8r9jv3jtN9z71ncpztOaXrdX8OHXRF5MeAfwb8OLAEvAe4D/iPIpIfdn93Im1whHZ4Jsf5cIQ58lTVA2/AIrALfN+r/j8ArgL/HfBLwE++4rH3AS9Of/9XQATG0/38I+B+QIEfBi4BLwH/8BWvP9T+DuPTVse2eCbH+XCcN0/H4fgOoAP85iv/qaq7IvI7wF8Bytd7sar+gIi8F/ioqv4nABG5f/rwE8AjwIPA/yMin997zmH2dwtogyO0wzM5zocjzJHnYZsXVoF1VfWv8dhL08dvlv9NVYeq+kXg48D3v4l9vRna4Ajt8EyO8+EIc+R52KC7DqyKyGvVkM9MH79ZXnjF788DZ9/Evt4MbXCEdngmx/lwhDnyPGzQ/QxNFf5vvvKfIjIAPgD8Z2AI9F7x8OlX7UNfZ9/3vOL3e2naWHgT+7tZ2uAI7fBMjvPhCPPkeRMN2v8IWAP+GpDRNEb/DvBZoAD+HvBV4Nj0JP8L08bn6ev/C/DDr/j7/unJ/+up4OPAFeD908cPtb9b1Gg/945t8UyO8+E4T543K//fA1+i6blbA/4FsDJ9rAP8GrAN/AnwP77qRD8EXAA2gX/IN/YgXuYVPYGH3d8t/IDn3rEtnslxPhznxVOmL75tTHsQnwUyfe1G8rueNjhCOzyT4/xwuzzTNOBEIpGYISnoJhKJxAy57c0LiUQi0SZSTTeRSCRmSAq6iUQiMUP2zb0gInd124OqykGe1wbP5Hjnk67Xl5lnx1TTTSQSiRmSgm4ikUjMkBR0E4lEYoakoJtIJBIzJAXdRCKRmCEp6CYSicQMSUE3kZhiTFMcRA40cuuupQ2ed7LjYddISyTmjuPHj/P93//9fOADH2BtbY2lpSWefPJJPvGJT/DMM89w5coVQgi3+zTfNG3wvCsc3yB3pd7N2yFydN72cz1qz9t9jneq43d8x3foH/3RH2mMUV9NVVV6+fJl/djHPqbnz5+/Ixzb4jnPjq0vqG3xvN3neCc6vu1tb9PnnnvuGwroa/H000/rRz7yEbXW3lbHtnjOs2PrC2pbPG/3Od5pjsYY/fjHP36gQrrHzs6OfuxjH9PHH39cFxYW7orr9W71nGfH1hfUtnje7nO80xyPHz+uFy5cOFRB3WN7e1s///nP6/ve9747/nq9Wz3n2bH1BbUtnrf7HO80x1OnTuna2tpNFdQ9Ll68qN/+7d9+R1+vd6vnPDumIWOJViIib3o40dmzZ/mpn/opVlZWbtFZ3Xra4Hm3Oaagm2gljz32GEtLS296P0888QQ/93M/R6/XuwVndetpg+dd53irqvh34naQ25i2eN7uc7yTHJ1z+uu//utv6nb0lXjv9R//43+sxpg76nq9mz3n2fG2FtQ8z3VlZUWLorhtBbUtnsnx5e3RRx/Vzc3NW1ZQVVW3trb0u77ru+6o6/Vu9pxnx5nMSMvznFOnTvHwww+zuLjIeDxmcXGRj370ozz66KNcunSJX/3VX+UXf/EXGY/HszilI6ENnvPg+P73v/+W3I6+ksXFRX7yJ3+S7/3e72Vra+uW7vtmaYPnXel4K75tXm/r9Xr6oz/6o/rpT39a19bW1HuvIQQNIXzDrBHvvX784x/X48ePz7R21BbP5Pjy9k/+yT+5ifrPGxNC0B/7sR+7Y67Xu9lznh2PrKAaY/Sf/tN/qt77A4vGGPVf/st/+abbU47qIr6bPZNjs4mI/tt/+28P7HFYnn32Wb3//vtv+/V6t3vOs+ORFdS3ve1tN9XW8vzzz+vp06dnVlDb4pkcX95+5Vd+5dAuh+G3fuu3dHl5+bZfr3ez5zw7HtmQsccee4zFxcVDv+6ee+7hh3/4h+/IlGyvRRs8583x6aefPtL9/42/8Tf4hV/4Bfr9/pEe541og+fd6HhkQfcd73jHTRU2EeFHfuRHeOSRR47grG49bfCcN8fPfe5zR5reT0T4yEc+woc//OEjO8ZBaIPn3eh4ZEH37NmzN/3aEydO8MEPfvAWns3R0QbPeXN8+umnj3xkhbWWD33oQ7e1lt8Gz7vR8Y6dkfbOd77zdp/CTGiD553mePHiRZ577rkjP85jjz12W2dwtcHzbnQ8sqD7J3/yJ2/q9YuLizeW3LiTaYPnvDkOh0M+97nPHflxzp8/z4kTJ478OK9HGzzvRscjKwlf/OIXiTHe9OvvpEK6H23wnDdHVeUzn/nMkR/HOUdRFEd+nNejDZ53o+ORlYbLly8zGo1u6rUxRn77t3/7TRX0WdEGz3l0fPrpp6mq6kiP8cILL/DSSy8d6THeiDZ43nWOt2Ks3GttCwsL+tRTT93U2Lh/9+/+3S3J5r6fW9s8k+Of3brdrv7+7//+TTkdhBij/viP//htv17vVs95djyyguqc01/7tV97zUXi9uPJJ5/UU6dOvelCelQX8d3qmRy/cfuhH/ohDSEcyumgXLhw4dATQw7i2BbPeXY8soIKzTIaP/3TP61lWR5Y8Jd/+ZdvSSE9yov4bvRMjt+4LS0tHVkN6d/8m3+jInJHXK93o+c8Ox5pQYUm5d9v/uZvHljwYx/72EwLals8k+Nrb29729v0+eefP7DTQaiqSj/0oQ/dUdfr3eY5z45H3q1cVRWf+tSnDvTcEAKf/OQnj/aEjog2eM6j45e+9CU++tGPsra2dsv2+bu/+7t84hOfuGX7uxW0wfOucbzV3zavtb373e/W69evv+G3yu/93u9pv9+fae2oLZ7Jcf/tiSee0GeeeeYNvd6InZ0d/c7v/M479nq9Wzzn2XEmBVVE9Ad/8Af16tWrrylXlqX+wR/8gb71rW+9ZYV0Vhfx3eKZHN94u//++/VnfuZn9MKFCzfdKfPzP//zaq29o6/Xu8Fznh1lKviaTBuPbwkiwuOPP873fd/38d73vpd7772XS5cu8eUvf5lf//Vf5zOf+QzD4fBWHQ4AVT3QZOk2eCbHgyEinD59mu/+7u/mB37gB3jf+9534EHxX/rSl3j/+99/0+M5Z3m93umec+14lN82r7c553RxcVGzLDuS/e9tB/lGbYtncjz8lue5/p2/83f00qVLb1grunTpkj7xxBN35fV6J3rOs+NtKaiz2m7XRXwnet7uc7ybHd/znvfoZz7zGa3r+hsK6LVr1/RXf/VX9du+7dtm4tgWz3l2nFnzwu1Ab0Pzwu3gIJ7J8c2xsLDAu9/9bt71rnfxrne9i6WlJT75yU/yG7/xGzzzzDO3JKfrnXC93ime8+yYgi7t8EyOtw4RQURueT6JO+16vZ2e8+w4kyXYE4l54hW3wHNNGzxvh+OdlXMvkUgk5pwUdBOJRGKGpKCbSCQSMyQF3UQikZghKegmEonEDElBN5FIJGbIvuN0E4lEInFrSTXdRCKRmCEp6CYSicQMSUE3kUgkZkgKuolEIjFDUtBNJBKJGZKCbiKRSMyQ/x8T8nfTrGe8LAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=bce_loss,\n",
    "                  optimizer=optimizer,\n",
    "                  training_DataLoader=train_loader,\n",
    "                  validation_DataLoader=validation_loader,\n",
    "                  lr_scheduler=None,\n",
    "                  epochs=50,\n",
    "                  epoch=0,\n",
    "                  notebook=True)\n",
    "\n",
    "# start training\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6338476747274399"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7ff7cc915750>, <torch.cuda.device at 0x7ff7cc868690>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_devices = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111351/3221111721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_losses' is not defined"
     ]
    }
   ],
   "source": [
    "validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
